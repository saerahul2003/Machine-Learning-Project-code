# -*- coding: utf-8 -*-
"""MLProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jSbgPTsYLSIApfVpAq1T5X3XvVnDgLgc
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import re
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

data = pd.read_csv('flipkart_data.csv')
data.head()

# unique ratings
pd.unique(data['rating'])

sns.countplot(data=data,
			x='rating',
			order=data.rating.value_counts().index)

# rating label(final)
pos_neg = []
for i in range(len(data['rating'])):
	if data['rating'][i] >= 5:
		pos_neg.append(1)
	else:
		pos_neg.append(0)

data['label'] = pos_neg

from tqdm import tqdm


def preprocess_text(text_data):
	preprocessed_text = []

	for sentence in tqdm(text_data):
		# Removing punctuations
		sentence = re.sub(r'[^\w\s]', '', sentence)

		# Converting lowercase and removing stopwords
		preprocessed_text.append(' '.join(token.lower()
										for token in nltk.word_tokenize(sentence)
										if token.lower() not in stopwords.words('english')))

	return preprocessed_text



data.head()

data["label"].value_counts()

data["label"].value_counts()

consolidated = ' '.join(
	word for word in data['review'][data['label'] == 1].astype(str))
wordCloud = WordCloud(width=1600, height=800,
					random_state=21, max_font_size=110)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

cv = TfidfVectorizer(max_features=2500)
X = cv.fit_transform(data['review'] ).toarray()

X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, data['label'],
													test_size=0.33,
													stratify=data['label'],
													random_state = 42)

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state=0)
model.fit(X_train,y_train)

#testing the model
pred = model.predict(X_train)
print((y_train,pred))

from sklearn import metrics
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_train,pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,
											display_labels = [False, True])

cm_display.plot()
plt.show()

from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score

print("\nAccuracy:", accuracy_score(y_train, pred))
print("\nPrecision:", precision_score(y_train, pred))
print("\nRecall:", recall_score(y_train, pred))
print("\nF1 Score:", f1_score(y_train, pred))

print("\nClassification Report:\n", classification_report(y_train, pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
import numpy as np

# Random Forest model
rf_model = RandomForestClassifier(random_state=0)
rf_model.fit(X_train, y_train)

print("\nDecision Tree Model Performance:")
print("Accuracy:", accuracy_score(y_train, pred))
print("Precision:", precision_score(y_train, pred))
print("Recall:", recall_score(y_train, pred))
print("F1 Score:", f1_score(y_train, pred))

y_pred_prob_dt = model.predict_proba(X_train)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_train, y_pred_prob_dt)
auc_dt = roc_auc_score(y_train, y_pred_prob_dt)

# Plot ROC Curve for Decision Tree
plt.figure(figsize=(8, 6))
plt.plot(fpr_dt, tpr_dt, label='Decision Tree (AUC = %0.2f)' % auc_dt)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Evaluate Random Forest model
y_pred_rf = rf_model.predict(X_train)
print("\nRandom Forest Model Performance:")
print("Accuracy:", accuracy_score(y_train, y_pred_rf))
print("Precision:", precision_score(y_train, y_pred_rf))
print("Recall:", recall_score(y_train, y_pred_rf))
print("F1 Score:", f1_score(y_train, y_pred_rf))

# ROC Curve and AUC for Random Forest
y_pred_prob_rf = rf_model.predict_proba(X_train)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_train, y_pred_prob_rf)
auc_rf = roc_auc_score(y_train, y_pred_prob_rf)

# Plot ROC Curve for Random Forest
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = %0.2f)' % auc_rf)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Feature importances for Random Forest
feature_importances = rf_model.feature_importances_
feature_names = cv.get_feature_names_out()

# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Plot feature importances
plt.figure(figsize=(12, 6))
sns.barplot(x=feature_importances[indices][:20], y=np.array(feature_names)[indices][:20])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Top 20 Features Importance - Random Forest')
plt.show()

from sklearn.linear_model import LogisticRegression

# Logistic Regression model
lr_model = LogisticRegression(random_state=0)
lr_model.fit(X_train, y_train)

# Evaluate Logistic Regression model
y_pred_lr = lr_model.predict(X_train)
print("\nLogistic Regression Model Performance:")
print("Accuracy:", accuracy_score(y_train, y_pred_lr))
print("Precision:", precision_score(y_train, y_pred_lr))
print("Recall:", recall_score(y_train, y_pred_lr))
print("F1 Score:", f1_score(y_train, y_pred_lr))

# ROC Curve and AUC for Logistic Regression
y_pred_prob_lr = lr_model.predict_proba(X_train)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_train, y_pred_prob_lr)
auc_lr = roc_auc_score(y_train, y_pred_prob_lr)

# Plot ROC Curve for Logistic Regression
plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, label='Logistic Regression (AUC = %0.2f)' % auc_lr)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Compare models
print("\nComparison of Models:")
print("Decision Tree - Accuracy:", accuracy_score(y_train, pred))
print("Random Forest - Accuracy:", accuracy_score(y_train, y_pred_rf))
print("Logistic Regression - Accuracy:", accuracy_score(y_train, y_pred_lr))







